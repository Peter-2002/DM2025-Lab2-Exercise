{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: Kittiphoom Treesam(å–†æ¡‘)\n",
    "\n",
    "Student ID: 111072281   \n",
    "\n",
    "GitHub ID: Peter-2002\n",
    "\n",
    "Kaggle name: Kittiphoom Treesam\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Styler' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Display Training Set\u001b[39;00m\n\u001b[32m     69\u001b[39m styled_train = (df_train[[\u001b[33m'\u001b[39m\u001b[33mpost_id\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33memotion\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     70\u001b[39m                 .rename(columns={\u001b[33m'\u001b[39m\u001b[33mpost_id\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mTokens\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33memotion\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mEmotion\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     71\u001b[39m                 .style\n\u001b[32m     72\u001b[39m                 .set_table_styles(table_style)\n\u001b[32m     73\u001b[39m                 .set_caption(\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š Training Data (First 10 Rows)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     74\u001b[39m                 .hide(axis=\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m display(\u001b[43mstyled_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m(\u001b[32m10\u001b[39m))\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Display Test Set\u001b[39;00m\n\u001b[32m     78\u001b[39m styled_test = (df_test[[\u001b[33m'\u001b[39m\u001b[33mpost_id\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     79\u001b[39m                .rename(columns={\u001b[33m'\u001b[39m\u001b[33mpost_id\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mTokens\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     80\u001b[39m                .style\n\u001b[32m     81\u001b[39m                .set_table_styles(table_style)\n\u001b[32m     82\u001b[39m                .set_caption(\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š Test Data (First 10 Rows)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m                .hide(axis=\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'Styler' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import display\n",
    "\n",
    "# --- File Paths ---\n",
    "POSTS_FILE = 'final_posts.json'\n",
    "ID_FILE = 'data_identification.csv'\n",
    "EMOTION_FILE = 'emotion.csv'\n",
    "\n",
    "# --- NLTK & Emoji Setup ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, return_tokens=False):\n",
    "    if pd.isna(text) or text is None:\n",
    "        return [] if return_tokens else \"\"\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return lemmas if return_tokens else \" \".join(lemmas)\n",
    "\n",
    "# --- Load and Merge Data ---\n",
    "with open(POSTS_FILE, 'r', encoding='utf-8') as f:\n",
    "    posts_data = [item['root']['_source']['post'] for item in json.load(f)]\n",
    "df_posts = pd.DataFrame(posts_data)\n",
    "\n",
    "df_id = pd.read_csv(ID_FILE)\n",
    "df_emotion = pd.read_csv(EMOTION_FILE)\n",
    "\n",
    "# Merge ID & Emotion\n",
    "df_labels = pd.merge(df_id, df_emotion, on='id', how='left').rename(columns={'id':'post_id'})\n",
    "\n",
    "# Merge Posts with Labels\n",
    "df_master = pd.merge(df_posts, df_labels, on='post_id', how='left')\n",
    "df_master.drop(columns=['hashtags'], inplace=True)\n",
    "\n",
    "# Split Train/Test\n",
    "df_train = df_master[df_master['split']=='train'].reset_index(drop=True)\n",
    "df_test = df_master[df_master['split']=='test'].reset_index(drop=True)\n",
    "df_train.drop(columns=['split'], inplace=True)\n",
    "df_test.drop(columns=['split', 'emotion'], inplace=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train['tokens'] = df_train['text'].apply(lambda x: preprocess_text(x, return_tokens=True))\n",
    "df_test['tokens'] = df_test['text'].apply(lambda x: preprocess_text(x, return_tokens=True))\n",
    "\n",
    "# --- Table Styling ---\n",
    "table_style = [\n",
    "    dict(selector='th', props=[('background-color', '#cce5ff'), ('color', '#000'), ('text-align','left'),\n",
    "                               ('font-weight','bold'), ('padding','8px')]),\n",
    "    dict(selector='td', props=[('text-align','left'),('padding','6px'),('vertical-align','top'),\n",
    "                               ('white-space','pre-wrap'),('max-width','350px')]),\n",
    "    dict(selector='tr:nth-child(even)', props=[('background-color','#f9f9f9')]),\n",
    "    dict(selector=\"tbody tr:hover\", props=[(\"background-color\", \"#e6f7ff\")])\n",
    "]\n",
    "\n",
    "# Display Training Set\n",
    "styled_train = (df_train[['post_id','text','emotion','tokens']]\n",
    "                .rename(columns={'post_id':'ID','text':'Text','tokens':'Tokens','emotion':'Emotion'})\n",
    "                .style\n",
    "                .set_table_styles(table_style)\n",
    "                .set_caption(\"ðŸ“Š Training Data (First 10 Rows)\")\n",
    "                .hide(axis=\"index\"))\n",
    "display(styled_train.)\n",
    "\n",
    "# Display Test Set\n",
    "styled_test = (df_test[['post_id','text','tokens']]\n",
    "               .rename(columns={'post_id':'ID','text':'Text','tokens':'Tokens'})\n",
    "               .style\n",
    "               .set_table_styles(table_style)\n",
    "               .set_caption(\"ðŸ“Š Test Data (First 10 Rows)\")\n",
    "               .hide(axis=\"index\"))\n",
    "display(styled_test.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. SIMULATING TRAINING PHASE (Word2Vec and Model Fitting) ---\n",
      "Simulated model trained successfully. Combined feature vector size: 104\n",
      "\n",
      "--- 2. Feature Engineering on Test File: final_posts.json ---\n",
      "Test feature matrix shape: (64171, 104)\n",
      "\n",
      "--- 3. Prediction and Submission Generation ---\n",
      "\n",
      "âœ… Submission file created successfully: 'DM2025_Lab2_Submission_Word2Vec_Emoji.csv'\n",
      "Prediction Sample (Top 5):\n",
      "          id   emotion\n",
      "0   0x61fc95  surprise\n",
      "1   0x35663e  surprise\n",
      "2   0xc78afe     anger\n",
      "3   0x90089c  surprise\n",
      "4   0xaba820  surprise\n",
      "5   0x66e44d  surprise\n",
      "6   0xc03cf5  surprise\n",
      "7   0x2ffb63     anger\n",
      "8   0x02f65a  surprise\n",
      "9   0x989146  surprise\n",
      "10  0x111ebf  surprise\n",
      "11  0xb9a96c     anger\n",
      "12  0xcc5e81  surprise\n",
      "13  0x08f6d6  surprise\n",
      "14  0xb855e1  surprise\n",
      "15  0xaa3a20  surprise\n",
      "16  0x479407  surprise\n",
      "17  0x784799  surprise\n",
      "18  0xd64e57  surprise\n",
      "19  0x77f7de     anger\n",
      "20  0xe07a21  surprise\n",
      "21  0x0efb36  surprise\n",
      "22  0xd97dc1     anger\n",
      "23  0x5bbbaf  surprise\n",
      "24  0x0f6fe1  surprise\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import emoji # Needed for robust emoji handling\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- NLTK Imports ---\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# --- 0. Initialization & Emoji Lexicon ---\n",
    "\n",
    "#initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "SCALER = StandardScaler() # Used to normalize statistical features\n",
    "\n",
    "# Simplified Emoji Lexicon: Map common emojis to simple numeric sentiment scores\n",
    "EMOJI_SENTIMENT_MAP = {\n",
    "    'ðŸ˜‚': 0.8, 'ðŸ˜­': -0.7, 'ðŸ˜Š': 0.9, 'â¤ï¸': 0.9, 'ðŸ‘': 0.7, \n",
    "    'ðŸ˜”': -0.8, 'ðŸ˜ ': -0.9, 'ðŸ˜¡': -0.9, 'ðŸ˜¢': -0.8, 'ðŸ˜': 0.9,\n",
    "    'ðŸ¤”': 0.1, 'ðŸ˜«': -0.6, 'ðŸ¥³': 1.0, 'ðŸŽ‰': 0.8, 'ðŸ˜': 0.0,\n",
    "    # Adding common ones related to your competition emotions:\n",
    "    'ðŸ˜¨': -0.9, 'ðŸ˜±': -0.9, 'ðŸ¤¯': 0.7, 'ðŸ¤¢': -0.8\n",
    "}\n",
    "\n",
    "# --- Pre-processing Function ---\n",
    "\n",
    "def preprocess_text(text, return_tokens=False):\n",
    "    \"\"\"Cleans text, converts emojis to their text description, and tokenizes.\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        if return_tokens: return []\n",
    "        return \"\"\n",
    "        \n",
    "    text = text.lower()\n",
    "    \n",
    "    #convert Emojis to text description (e.g., \"ðŸ˜Š\" -> \" smiling_face_with_smiling_eyes \")\n",
    "    #this keeps the semantic meaning for Word2Vec training\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \")) \n",
    "    \n",
    "    #clean up noise (mentions, URLs, punctuation)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuation (now that emojis are converted)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #stop Word Removal and Lemmatization\n",
    "    lemmas = [\n",
    "        lemmatizer.lemmatize(w) \n",
    "        for w in tokens if w not in stop_words\n",
    "    ]\n",
    "    \n",
    "    return lemmas if return_tokens else \" \".join(lemmas)\n",
    "\n",
    "# --- Word2Vec Helper Function (No change) ---\n",
    "def average_word_vectors(tokens, model, vector_size):\n",
    "    \"\"\"Averages the word vectors for all tokens in a document.\"\"\"\n",
    "    if not tokens: return np.zeros(vector_size)\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if not vectors: return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# --- Feature Calculation Helper (NEW) ---\n",
    "def calculate_emoji_features(raw_text):\n",
    "    \"\"\"Calculates emoji count and average sentiment based on the predefined lexicon.\"\"\"\n",
    "    if pd.isna(raw_text) or raw_text is None:\n",
    "        return 0, 0\n",
    "    \n",
    "    #extract all emojis\n",
    "    all_emojis = [c for c in raw_text if c in emoji.EMOJI_DATA]\n",
    "    count = len(all_emojis)\n",
    "    \n",
    "    if count == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    #calculate average sentiment based on the EMOJI_SENTIMENT_MAP\n",
    "    sentiment_sum = sum(EMOJI_SENTIMENT_MAP.get(e, 0) for e in all_emojis)\n",
    "    average_sentiment = sentiment_sum / count\n",
    "    \n",
    "    return count, average_sentiment\n",
    "\n",
    "\n",
    "# --- START: SIMULATION OF TRAINING PHASE ---\n",
    "VECTOR_SIZE = 100 \n",
    "DUMMY_TRAIN_SIZE = 5000 \n",
    "#(Dummy Data Creation omitted for brevity, but should remain consistent)\n",
    "\n",
    "print(\"--- 1. SIMULATING TRAINING PHASE (Word2Vec and Model Fitting) ---\")\n",
    "\n",
    "# --- Dummy Data Setup (Ensure text now includes emojis for proper W2V training) ---\n",
    "DUMMY_TRAIN_DF = pd.DataFrame({\n",
    "    'post_id': [f'0x{i:05d}' for i in range(DUMMY_TRAIN_SIZE)],\n",
    "    'text': ['I am so happy today! ðŸ˜Š' if i % 3 == 0 else \n",
    "             'This is frustrating ðŸ˜  and boring.' if i % 3 == 1 else \n",
    "             'What a surprise! ðŸ¤¯' for i in range(DUMMY_TRAIN_SIZE)],\n",
    "    'hashtags': [[]] * DUMMY_TRAIN_SIZE,\n",
    "    'emotion': np.random.choice(['joy', 'anger', 'surprise'], size=DUMMY_TRAIN_SIZE)\n",
    "})\n",
    "\n",
    "\n",
    "#pre-process for Word2Vec (returning tokens)\n",
    "DUMMY_TRAIN_DF['tokens'] = DUMMY_TRAIN_DF['text'].apply(lambda x: preprocess_text(x, return_tokens=True))\n",
    "\n",
    "#fit the Word2Vec Model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=DUMMY_TRAIN_DF['tokens'], vector_size=VECTOR_SIZE, window=5, min_count=5, workers=4\n",
    ")\n",
    "\n",
    "#calculate Word2Vec Features\n",
    "X_train_w2v_raw = np.array([\n",
    "    average_word_vectors(tokens, w2v_model, VECTOR_SIZE)\n",
    "    for tokens in DUMMY_TRAIN_DF['tokens']\n",
    "])\n",
    "\n",
    "#calculate Lexical/Statistical Features for Training Data\n",
    "DUMMY_TRAIN_DF['char_count'] = DUMMY_TRAIN_DF['text'].str.len().fillna(0)\n",
    "DUMMY_TRAIN_DF['vader_compound'] = DUMMY_TRAIN_DF['text'].astype(str).apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "DUMMY_TRAIN_DF[['emoji_count', 'emoji_sentiment']] = DUMMY_TRAIN_DF['text'].apply(\n",
    "    lambda x: pd.Series(calculate_emoji_features(x))\n",
    ")\n",
    "\n",
    "X_train_stat_raw = DUMMY_TRAIN_DF[[\n",
    "    'char_count', 'vader_compound', 'emoji_count', 'emoji_sentiment'\n",
    "]].values\n",
    "\n",
    "#normalize Statistical Features\n",
    "X_train_stat_scaled = SCALER.fit_transform(X_train_stat_raw) \n",
    "\n",
    "#combine Features\n",
    "X_train_combined = np.hstack([X_train_w2v_raw, X_train_stat_scaled])\n",
    "\n",
    "#train the LinearSVC Model\n",
    "y_train = DUMMY_TRAIN_DF['emotion']\n",
    "model = LinearSVC(C=0.5, random_state=42, max_iter=2000)\n",
    "model.fit(X_train_combined, y_train)\n",
    "print(f\"Simulated model trained successfully. Combined feature vector size: {X_train_combined.shape[1]}\")\n",
    "# --- END: SIMULATION OF TRAINING PHASE ---\n",
    "\n",
    "\n",
    "#LOADING AND FEATURE ENGINEERING FOR YOUR TEST DATA (final_posts.json) ---\n",
    "\n",
    "file_path_test = 'final_posts.json'\n",
    "print(f\"\\n--- 2. Feature Engineering on Test File: {file_path_test} ---\")\n",
    "\n",
    "#load your actual submission data (Test Set)\n",
    "# (Loading and flattening df_test is omitted for brevity but remains the same)\n",
    "try:\n",
    "    with open(file_path_test, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Test file not found at '{file_path_test}'.\")\n",
    "    exit()\n",
    "test_posts_data = [item['root']['_source']['post'] for item in test_data]\n",
    "df_test = pd.DataFrame(test_posts_data)\n",
    "\n",
    "\n",
    "#apply Pre-processing to Test Data (returning tokens)\n",
    "df_test['tokens'] = df_test['text'].apply(lambda x: preprocess_text(x, return_tokens=True))\n",
    "\n",
    "#calculate Word2Vec Features\n",
    "X_test_w2v_raw = np.array([\n",
    "    average_word_vectors(tokens, w2v_model, VECTOR_SIZE)\n",
    "    for tokens in df_test['tokens']\n",
    "])\n",
    "\n",
    "#calculate Statistical Features for Test Data (INCLUDING EMOJIS)\n",
    "df_test['char_count'] = df_test['text'].str.len().fillna(0)\n",
    "df_test['vader_compound'] = df_test['text'].astype(str).apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df_test[['emoji_count', 'emoji_sentiment']] = df_test['text'].apply(\n",
    "    lambda x: pd.Series(calculate_emoji_features(x))\n",
    ")\n",
    "\n",
    "X_test_stat_raw = df_test[['char_count', 'vader_compound', 'emoji_count', 'emoji_sentiment']].values\n",
    "\n",
    "#scale Statistical Features\n",
    "X_test_stat_scaled = SCALER.transform(X_test_stat_raw)\n",
    "\n",
    "#combine Features\n",
    "X_test_combined = np.hstack([X_test_w2v_raw, X_test_stat_scaled])\n",
    "\n",
    "print(f\"Test feature matrix shape: {X_test_combined.shape}\")\n",
    "\n",
    "#PREDICTION AND SUBMISSION GENERATION ---\n",
    "\n",
    "print(\"\\n--- 3. Prediction and Submission Generation ---\")\n",
    "\n",
    "final_predictions = model.predict(X_test_combined)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': df_test['post_id'],\n",
    "    'emotion': final_predictions\n",
    "})\n",
    "\n",
    "#save the submission file\n",
    "submission_file_path = 'DM2025_Lab2_Submission_Word2Vec_Emoji.csv'\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Submission file created successfully: '{submission_file_path}'\")\n",
    "print(f\"Prediction Sample (Top 5):\")\n",
    "print(submission_df.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Training Setup and Feature Engineering ---\n",
      "Total training features created: 104 (100 W2V + 4 Stat)\n",
      "\n",
      "--- 2. Model Implementation ---\n",
      "Data split: Training size=4000, Test size=1000\n",
      "Starting Grid Search for LinearSVC C parameter...\n",
      "âœ… Best Hyperparameter found: C=0.1\n",
      "\n",
      "--- 3. Model Evaluation on Simulated Test Set ---\n",
      "Classifier: LinearSVC\n",
      "F1 Score (Macro Avg): 0.2741\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       332\n",
      "         joy       0.35      0.71      0.47       342\n",
      "    surprise       0.36      0.34      0.35       326\n",
      "\n",
      "    accuracy                           0.35      1000\n",
      "   macro avg       0.24      0.35      0.27      1000\n",
      "weighted avg       0.24      0.35      0.28      1000\n",
      "\n",
      "\n",
      "--- 4. Prediction on Final Submission Data (final_posts.json) ---\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import emoji\n",
    "import nltk\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # <-- ADDED GridSearchCV for tuning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from gensim.models import Word2Vec \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# --- 0. Initialization & NLTK/Emoji Setup (Ensuring environment is ready) ---\n",
    "\n",
    "# Initialize NLTK tools (assuming data is downloaded, if not, add download logic)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "SCALER = StandardScaler() \n",
    "ENCODER = LabelEncoder() \n",
    "\n",
    "# Simplified Emoji Lexicon:\n",
    "EMOJI_SENTIMENT_MAP = {\n",
    "    'ðŸ˜‚': 0.8, 'ðŸ˜­': -0.7, 'ðŸ˜Š': 0.9, 'â¤ï¸': 0.9, 'ðŸ‘': 0.7, \n",
    "    'ðŸ˜”': -0.8, 'ðŸ˜ ': -0.9, 'ðŸ˜¡': -0.9, 'ðŸ˜¢': -0.8, 'ðŸ˜': 0.9,\n",
    "    'ðŸ¤”': 0.1, 'ðŸ˜«': -0.6, 'ðŸ¥³': 1.0, 'ðŸŽ‰': 0.8, 'ðŸ˜': 0.0,\n",
    "    'ðŸ˜¨': -0.9, 'ðŸ˜±': -0.9, 'ðŸ¤¯': 0.7, 'ðŸ¤¢': -0.8\n",
    "}\n",
    "VECTOR_SIZE = 100 \n",
    "\n",
    "\n",
    "# --- Pre-processing & Feature Helper Functions (From previous steps) ---\n",
    "\n",
    "def preprocess_text(text, return_tokens=False):\n",
    "    if pd.isna(text) or text is None:\n",
    "        if return_tokens: return []\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \")) \n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return lemmas if return_tokens else \" \".join(lemmas)\n",
    "\n",
    "def average_word_vectors(tokens, model, vector_size):\n",
    "    if not tokens: return np.zeros(vector_size)\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if not vectors: return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "def calculate_emoji_features(raw_text):\n",
    "    if pd.isna(raw_text) or raw_text is None: return 0, 0\n",
    "    all_emojis = [c for c in raw_text if c in emoji.EMOJI_DATA]\n",
    "    count = len(all_emojis)\n",
    "    if count == 0: return 0, 0\n",
    "    sentiment_sum = sum(EMOJI_SENTIMENT_MAP.get(e, 0) for e in all_emojis)\n",
    "    return count, sentiment_sum / count\n",
    "\n",
    "def calculate_statistical_features(df):\n",
    "    df['char_count'] = df['text'].str.len().fillna(0)\n",
    "    df['vader_compound'] = df['text'].astype(str).apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "    df[['emoji_count', 'emoji_sentiment']] = df['text'].apply(lambda x: pd.Series(calculate_emoji_features(x)))\n",
    "    return df[['char_count', 'vader_compound', 'emoji_count', 'emoji_sentiment']].values\n",
    "\n",
    "# --- 1. SIMULATION OF TRAINING DATA AND FEATURE ENGINEERING ---\n",
    "\n",
    "print(\"--- 1. Training Setup and Feature Engineering ---\")\n",
    "\n",
    "DUMMY_TRAIN_SIZE = 5000 \n",
    "DUMMY_TRAIN_DF = pd.DataFrame({\n",
    "    'post_id': [f'0x{i:05d}' for i in range(DUMMY_TRAIN_SIZE)],\n",
    "    'text': ['I am so happy today! ðŸ˜Š' if i % 3 == 0 else \n",
    "             'This is frustrating ðŸ˜  and boring.' if i % 3 == 1 else \n",
    "             'What a surprise! ðŸ¤¯' for i in range(DUMMY_TRAIN_SIZE)],\n",
    "    'emotion': np.random.choice(['joy', 'anger', 'surprise'], size=DUMMY_TRAIN_SIZE)\n",
    "})\n",
    "\n",
    "# Pre-process & Feature Generation\n",
    "DUMMY_TRAIN_DF['tokens'] = DUMMY_TRAIN_DF['text'].apply(lambda x: preprocess_text(x, return_tokens=True))\n",
    "w2v_model = Word2Vec(sentences=DUMMY_TRAIN_DF['tokens'], vector_size=VECTOR_SIZE, window=5, min_count=5, workers=4)\n",
    "\n",
    "X_train_w2v_raw = np.array([\n",
    "    average_word_vectors(tokens, w2v_model, VECTOR_SIZE)\n",
    "    for tokens in DUMMY_TRAIN_DF['tokens']\n",
    "])\n",
    "X_train_stat_raw = calculate_statistical_features(DUMMY_TRAIN_DF)\n",
    "\n",
    "# Transform the target variable (emotion strings to integers)\n",
    "y_train_raw = DUMMY_TRAIN_DF['emotion']\n",
    "y_encoded = ENCODER.fit_transform(y_train_raw)\n",
    "\n",
    "# Scale statistical features and combine all features\n",
    "X_train_stat_scaled = SCALER.fit_transform(X_train_stat_raw) \n",
    "X_combined = np.hstack([X_train_w2v_raw, X_train_stat_scaled])\n",
    "\n",
    "print(f\"Total training features created: {X_combined.shape[1]} (100 W2V + 4 Stat)\")\n",
    "\n",
    "\n",
    "# --- 2. MODEL IMPLEMENTATION (Training, Split, Evaluation) ---\n",
    "\n",
    "print(\"\\n--- 2. Model Implementation ---\")\n",
    "\n",
    "# 2.1 Train-Test Split (For evaluation purposes only)\n",
    "# Splitting the simulated data to check performance before using the model on the final submission file.\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(\n",
    "    X_combined, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "print(f\"Data split: Training size={len(X_TRAIN)}, Test size={len(X_TEST)}\")\n",
    "\n",
    "\n",
    "# 2.2 Hyperparameter Tuning (Optional but recommended)\n",
    "# We will use a quick Grid Search to find the best C parameter for LinearSVC.\n",
    "param_grid = {'C': [0.1, 0.5, 1.0]} # Smaller range for speed\n",
    "grid_search = GridSearchCV(\n",
    "    LinearSVC(max_iter=5000, random_state=42), \n",
    "    param_grid, \n",
    "    scoring='f1_macro', \n",
    "    cv=3, \n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Starting Grid Search for LinearSVC C parameter...\")\n",
    "grid_search.fit(X_TRAIN, Y_TRAIN)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"âœ… Best Hyperparameter found: C={grid_search.best_params_['C']}\")\n",
    "# [Image of Grid Search Cross-Validation]\n",
    "\n",
    "# 2.3 Model Evaluation\n",
    "# Evaluate the best model on the held-out simulated test set\n",
    "Y_pred = best_model.predict(X_TEST)\n",
    "\n",
    "print(\"\\n--- 3. Model Evaluation on Simulated Test Set ---\")\n",
    "print(f\"Classifier: {type(best_model).__name__}\")\n",
    "print(f\"F1 Score (Macro Avg): {f1_score(Y_TEST, Y_pred, average='macro'):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_TEST, Y_pred, target_names=ENCODER.classes_))\n",
    "\n",
    "\n",
    "# --- 3. FINAL PREDICTION ON UNSEEN DATA (final_posts.json) ---\n",
    "\n",
    "print(\"\\n--- 4. Prediction on Final Submission Data (final_posts.json) ---\")\n",
    "\n",
    "# Load your actual submission data (Test Set)\n",
    "file_path_test = 'final_posts.json'\n",
    "try:\n",
    "    with open(file_path_test, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Test file not found at '{file_path_test}'.\")\n",
    "    exit()\n",
    "test_posts_data = [item['root']['_source']['post'] for item in test_data]\n",
    "df_test = pd.DataFrame(test_posts_data)\n",
    "\n",
    "# Apply Feature Engineering (TRANSFORM only, DO NOT FIT)\n",
    "df_test['tokens'] = df_test['text'].apply(lambda x: preprocess_text(x, return_tokens=True))\n",
    "X_test_w2v_raw = np.array([average_word_vectors(tokens, w2v_model, VECTOR_SIZE) for tokens in df_test['tokens']])\n",
    "X_test_stat_raw = calculate_statistical_features(df_test)\n",
    "X_test_stat_scaled = SCALER.transform(X_test_stat_raw)\n",
    "\n",
    "# Combine features for final prediction\n",
    "X_test_combined = np.hstack([X_test_w2v_raw, X_test_stat_scaled])\n",
    "\n",
    "# Predict and Inverse Transform\n",
    "final_predictions_encoded = best_model.predict(X_test_combined)\n",
    "final_predictions_raw = ENCODER.inverse_transform(final_predictions_encoded)\n",
    "\n",
    "# Generate Submission File\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': df_test['post_id'],\n",
    "    'emotion': final_predictions_raw \n",
    "})\n",
    "\n",
    "submission_file_path = 'DM2025_Lab2_Submission_Final.csv'\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Final Submission file created successfully: '{submission_file_path}'\")\n",
    "print(f\"Prediction Sample (Top 5):\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
